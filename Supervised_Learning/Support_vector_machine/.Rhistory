my_data<-read.table("./data_in/peso_data.txt", header=T) #Lee los datos del archivo
my_data_male<-my_data[which(my_data$sexo=="male"),]
#Ajustamos modelos
y<-my_data_male$peso #la variable respuesta es el Peso
x1<-my_data_male$altura #variable x1 determinada por Altura
x2<-my_data_male$cintura #variable x2 determinada por Cintura
x3<-my_data_male$cadera #variable x3 determinada por Cadera
model1<-lm(y~x1)
model2<-lm(y~x2)
model3<-lm(y~x3)
summary(model1)
summary(model2)
summary(model3)
#Realizamos una visualizacion
par(mfrow = c( 1, 3 ), oma = c(0,0,2.5,0))
plot(x1,y,col=4,xlab="Altura",ylab="Peso", main="Peso vs. Altura")
abline(model1,col=2)
plot(x2,y,col=4,xlab="Cintura",ylab="Peso", main="Peso vs. Cintura")
abline(model2,col=2)
plot(x3,y,col=4,xlab="Cadera",ylab="Peso",  main="Peso vs. Cadera")
abline(model3,col=2)
title(main="Rectas de Regresi\u00f3n Lineal para cada variable explicativa",
outer=T,cex.main=2)
#Regresion lineal multiple
model4<-lm(peso~altura+cintura,data = my_data_male)
model5<-lm(peso~altura+cadera,data = my_data_male)
model6<-lm(peso~cintura+cadera,data = my_data_male)
model7<-lm(peso~altura+cintura+cadera,data = my_data_male)
summary(model4)
summary(model5)
summary(model6)
summary(model7)
# #Hacemos un grafico en 4D para sexo masculino y femenino con las variables altura y cintura
# p <- plot_ly(my_data, x = ~altura, y = ~cintura, z = ~peso ,color = ~sexo, colors = c('#BF382A', '#0C4B8E')) %>%
#   add_markers() %>%
#   layout(scene = list(xaxis = list(title = 'altura'),
#                       yaxis = list(title = 'cintura'),
#                       zaxis = list(title = 'peso')))
# p
#Hacemos un grafico en 3D para cintura y cadera añadiendo el plano
p <- plot_ly(my_df,
x = ~cintura,
y = ~cadera,
z = ~peso,
type = "scatter3d",
mode = "markers")
#Hacemos un grafico en 3D para cintura y cadera añadiendo el plano
p <- plot_ly(my_data_male,
x = ~cintura,
y = ~cadera,
z = ~peso,
type = "scatter3d",
mode = "markers")
p
#Graph Resolution (more important for more complex shapes)
graph_reso <- 0.05
#Setup Axis
axis_x <- seq(min(my_data_male$cintura), max(my_data_male$cintura), by = graph_reso)
axis_y <- seq(min(my_data_male$cadera), max(my_data_male$cadera), by = graph_reso)
#Sample points
lm_surface <- expand.grid(cintura = axis_x,cadera = axis_y,KEEP.OUT.ATTRS = F)
lm_surface
axis_y
length(axis_y)
length(axis_x)
#Sample points
lm_surface <- expand.grid(cintura = axis_x,cadera = axis_y)
lm_surface
tail(axis_y)
tail(lm_surface)
#Sample points
lm_surface <- expand.grid(cintura = axis_x,cadera = axis_y,KEEP.OUT.ATTRS = F)
lm_surface$peso <- predict(model6, newdata = lm_surface[1,],type = "response")
lm_surface$peso
taiñl(lm_surface$peso)
tail(lm_surface$peso)
table(unique(lm_surface$peso))
lm_surface$peso <- predict(model6, newdata = lm_surface,type = "response")
tail(lm_surface$peso)
lm_surface$peso <- predict(model6, newdata = lm_surface,type = "response")
lm_surface <- acast(lm_surface, cadera ~ cintura, value.var = "peso") #y ~ x
p<-  add_trace(p = p,
z = lm_surface,
x = axis_x,
y = axis_y,
type = "surface")
p
predictorvalues<-function(x1,x2,x3,type){
if (is.character(type)==T){
new<-data.frame(x1=x1,x2=x2,x3=x3)
predict.lm(model7,new,interval=type)
}
else print("Debe introducir una variable del tipo 'caracter', como por ejemplo
'confidences'") #condicionamiento de la funciÃ³n
}
predictorvalues<-function(x1,x2,x3,type,model){
if (is.character(type)==T){
new<-data.frame(x1=x1,x2=x2,x3=x3)
predict.lm(model,new,interval=type)
}
else print("Debe introducir una variable del tipo 'caracter', como por ejemplo
'confidences'") #condicionamiento de la funciÃ³n
}
predictorvalues(66,37.95,43,model7,"confidence")
else print("Debe introducir una variable del tipo 'character', como por ejemplo \n 'confidences'") #condicionamiento de la funciÃ³n
predictorvalues<-function(x1,x2,x3,model, type,){
if (is.character(type)==T){
new<-data.frame(x1=x1,x2=x2,x3=x3)
predict.lm(model,new,interval=type)
}
else print("Debe introducir una variable del tipo 'character', como por ejemplo \n 'confidences'") #condicionamiento de la funciÃ³n
}
predictorvalues(66,37.95,43,model7,"confidence")
predictorvalues<-function(x1,x2,x3,model, type,){
if (is.character(type)==T){
new<-data.frame(x1=x1,x2=x2,x3=x3)
predict.lm(model,new,interval=type)
}
else print("Debe introducir una variable del tipo 'character', como por ejemplo \n 'confidences'") #condicionamiento de la funciÃ³n
}
predictorvalues<-function(x1,x2,x3,model, type,){
if (is.character(type)==T){
new<-data.frame(x1=x1,x2=x2,x3=x3)
predict.lm(model,new,interval=type)
}
else print("Debe introducir una variable del tipo 'character', como por ejemplo \n 'confidences'") #condicionamiento de la funciÃ³n
}
predictorvalues<-function(x1,x2,x3,model, type){
if (is.character(type)==T){
new<-data.frame(x1=x1,x2=x2,x3=x3)
predict.lm(model,new,interval=type)
}
else print("Debe introducir una variable del tipo 'character', como por ejemplo \n 'confidences'") #condicionamiento de la funciÃ³n
}
predictorvalues(66,37.95,43,model7,"confidence")
predictorvalues<-function(x1,x2,x3,model, type){
if (is.character(type)==T){
new<-data.frame(x1=altura,x2=cintura,x3=cadera)
predict.lm(model,new,interval=type)
}
else print("Debe introducir una variable del tipo 'character', como por ejemplo \n 'confidences'") #condicionamiento de la funciÃ³n
}
predictorvalues(66,37.95,43,model7,"confidence")
predictorvalues<-function(x1,x2,x3,model, type){
if (is.character(type)==T){
new<-data.frame(altura=x1,cintura=x2,cadera=x3)
predict.lm(model,new,interval=type)
}
else print("Debe introducir una variable del tipo 'character', como por ejemplo \n 'confidences'") #condicionamiento de la funciÃ³n
}
predictorvalues(66,37.95,43,model7,"confidence")
predictorvalues(66,37.95,43,model7,"prediction")
rm(list=ls())
setwd("C:/Users/Henry N/Desktop/KSchool/my_course/day_1/Support_vector_machine")
rm(list=ls())
set.seed(10111)
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1, 1), c(10, 10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)
if (!require("e1071")){install.packages("e1071",verbose = F) ; library("e1071")}
dat = data.frame(x, y = as.factor(y))
svmfit = svm(y ~ ., data = dat, kernel = "linear", cost = 10, scale = FALSE)
print(svmfit)
plot(svmfit, dat)
make.grid = function(x, n = 75) {
grange = apply(x, 2, range)
x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
expand.grid(X1 = x1, X2 = x2)
}
xgrid = make.grid(x)
xgrid[1:10,]
ygrid = predict(svmfit, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
beta = drop(t(svmfit$coefs)%*%x[svmfit$index,])
beta0 = svmfit$rho
plot(xgrid, col = c("red", "blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
abline(beta0 / beta[2], -beta[1] / beta[2])
abline((beta0 - 1) / beta[2], -beta[1] / beta[2], lty = 2)
abline((beta0 + 1) / beta[2], -beta[1] / beta[2], lty = 2)
load(file = "ESL.mixture.rda")
load(file = "ESL.mixture.rda")
names(ESL.mixture)
ESL.mixture
rm(x, y)
attach(ESL.mixture)
plot(x, col = y + 1)
dat = data.frame(y = factor(y), x)
fit = svm(factor(y) ~ ., data = dat, scale = FALSE, kernel = "radial", cost = 5)
xgrid = expand.grid(X1 = px1, X2 = px2)
ygrid = predict(fit, xgrid)
plot(xgrid, col = as.numeric(ygrid), pch = 20, cex = .2)
points(x, col = y + 1, pch = 19)
func = predict(fit, xgrid, decision.values = TRUE)
func = attributes(func)$decision
xgrid = expand.grid(X1 = px1, X2 = px2)
ygrid = predict(fit, xgrid)
plot(xgrid, col = as.numeric(ygrid), pch = 20, cex = .2)
points(x, col = y + 1, pch = 19)
contour(px1, px2, matrix(func, 69, 99), level = 0, add = TRUE)
contour(px1, px2, matrix(func, 69, 99), level = 0.5, add = TRUE, col = "blue", lwd = 2)
rm(list=ls())
set.seed(10111)
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1, 1), c(10, 10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)
detach(ESL.mixture)
ESL.mixture
rm(list=ls())
set.seed(10111)
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1, 1), c(10, 10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)
dat = data.frame(x, y = as.factor(y))
svmfit = svm(y ~ ., data = dat, kernel = "linear", cost = 10, scale = FALSE)
print(svmfit)
plot(svmfit, dat)
make.grid = function(x, n = 75) {
grange = apply(x, 2, range)
x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
expand.grid(X1 = x1, X2 = x2)
}
xgrid = make.grid(x)
xgrid[1:10,]
ygrid = predict(svmfit, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
beta = drop(t(svmfit$coefs)%*%x[svmfit$index,])
beta0 = svmfit$rho
plot(xgrid, col = c("red", "blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
abline(beta0 / beta[2], -beta[1] / beta[2])
abline((beta0 - 1) / beta[2], -beta[1] / beta[2], lty = 2)
abline((beta0 + 1) / beta[2], -beta[1] / beta[2], lty = 2)
load(file = "ESL.mixture.rda")
names(ESL.mixture)
rm(x, y)
ESL.mixture$x
ESL.mixture$y
ESL.mixture$means
if (!require("ISLR")){install.packages("ISLR",verbose = F) ; library("ISLR")}
library(ISLR)
data(package="ISLR")
carseats<-Carseats
require(tree)
if (!require("tree")){install.packages("tree",verbose = F) ; library("tree")}
names(carseats)
hist(carseats$Sales)
High = ifelse(carseats$Sales<=8, "No", "Yes")
carseats = data.frame(carseats, High)
tree.carseats = tree(High~.-Sales, data=carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats, pretty = 0)
tree.carseats
set.seed(101)
train=sample(1:nrow(carseats), 250)
set.seed(101)
train=sample(1:nrow(carseats), 250)
tree.carseats = tree(High~.-Sales, carseats, subset=train)
plot(tree.carseats)
text(tree.carseats, pretty=0)
tree.pred = predict(tree.carseats, carseats[-train,], type="class")
with(carseats[-train,], table(tree.pred, High))
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass)
cv.carseats
plot(cv.carseats)
prune.carseats = prune.misclass(tree.carseats, best = 12)
plot(prune.carseats)
text(prune.carseats, pretty=0)
tree.pred = predict(prune.carseats, carseats[-train,], type="class")
with(carseats[-train,], table(tree.pred, High))
(74 + 39) / 150
if (!require("MASS")){install.packages("MASS",verbose = F) ; library("MASS")}
library(MASS)
data(package="MASS")
boston<-Boston
dim(boston)
names(boston)
nrow(boston)
if (!require("randomForest")){install.packages("randomForest",verbose = F) ; library("randomForest")}
rf.boston = randomForest(medv~., data = boston, subset = train)
rf.boston
oob.err = double(13)
test.err = double(13)
for(mtry in 1:13){
fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350)
oob.err[mtry] = fit$mse[350]
pred = predict(fit, boston[-train,])
test.err[mtry] = with(boston[-train,], mean( (medv-pred)^2 ))
}
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))
rm(list=ls())
data(package="MASS")
rm(list=ls())
boston<-Boston
boost.boston = gbm(medv~., data = boston[train,], distribution = "gaussian", n.trees = 10000, shrinkage = 0.01, interaction.depth = 4)
if (!require("gbm")){install.packages("gbm",verbose = F) ; library("gbm")}
plot(boost.boston,i="lstat")
plot(boost.boston,i="rm")
rm(list=ls())
boston<-Boston
boost.boston = gbm(medv~., data = boston[train,], distribution = "gaussian", n.trees = 10000, shrinkage = 0.01, interaction.depth = 4)
rm(list=ls())
boston<-Boston
set.seed(101)
rm(list=ls())
boston<-Boston
set.seed(101)
train = sample(1:nrow(boston), 300)
boost.boston = gbm(medv~., data = boston[train,], distribution = "gaussian", n.trees = 10000, shrinkage = 0.01, interaction.depth = 4)
summary(boost.boston)
plot(boost.boston,i="lstat")
plot(boost.boston,i="rm")
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.boston, newdata = boston[-train,], n.trees = n.trees)
dim(predmat)
boost.err = with(boston[-train,], apply( (predmat - medv)^2, 2, mean) )
plot(n.trees, boost.err, pch = 23, ylab = "Mean Squared Error", xlab = "# Trees", main = "Boosting Test Error")
abline(h = min(test.err), col = "red")
rm(list=ls())
library(MASS)
data(package="MASS")
boston<-Boston
dim(boston)
names(boston)
set.seed(101)
train = sample(1:nrow(boston), 300)
rf.boston = randomForest(medv~., data = boston, subset = train)
rf.boston
oob.err = double(13)
test.err = double(13)
for(mtry in 1:13){
fit = randomForest(medv~., data = boston, subset=train, mtry=mtry, ntree = 350)
oob.err[mtry] = fit$mse[350]
pred = predict(fit, boston[-train,])
test.err[mtry] = with(boston[-train,], mean( (medv-pred)^2 ))
}
matplot(1:mtry, cbind(test.err, oob.err), pch = 23, col = c("red", "blue"), type = "b", ylab="Mean Squared Error")
legend("topright", legend = c("OOB", "Test"), pch = 23, col = c("red", "blue"))
################
boost.boston = gbm(medv~., data = boston[train,], distribution = "gaussian", n.trees = 10000, shrinkage = 0.01, interaction.depth = 4)
summary(boost.boston)
plot(boost.boston,i="lstat")
plot(boost.boston,i="rm")
n.trees = seq(from = 100, to = 10000, by = 100)
predmat = predict(boost.boston, newdata = boston[-train,], n.trees = n.trees)
dim(predmat)
boost.err = with(boston[-train,], apply( (predmat - medv)^2, 2, mean) )
plot(n.trees, boost.err, pch = 23, ylab = "Mean Squared Error", xlab = "# Trees", main = "Boosting Test Error")
abline(h = min(test.err), col = "red")
if (!require("splines")){install.packages("splines",verbose = F) ; library("splines")}
attach(Wage) #attaching Wage dataset
?Wage #for more details on the dataset
agelims<-range(age)
#Generating Test Data
age.grid<-seq(from=agelims[1], to = agelims[2])
#3 cutpoints at ages 25 ,50 ,60
fit<-lm(wage ~ bs(age,knots = c(25,40,60)),data = Wage )
summary(fit)
#Plotting the Regression Line to the scatterplot
plot(age,wage,col="grey",xlab="Age",ylab="Wages")
points(age.grid,predict(fit,newdata = list(age=age.grid)),col="darkgreen",lwd=2,type="l")
#adding cutpoints
abline(v=c(25,40,60),lty=2,col="darkgreen")
rm(list=ls())
attach(Wage) #attaching Wage dataset
?Wage #for more details on the dataset
agelims<-range(age)
#Generating Test Data
age.grid<-seq(from=agelims[1], to = agelims[2])
#3 cutpoints at ages 25 ,50 ,60
fit<-lm(wage ~ bs(age,knots = c(25,40,60)),data = Wage )
summary(fit)
#Plotting the Regression Line to the scatterplot
plot(age,wage,col="grey",xlab="Age",ylab="Wages")
points(age.grid,predict(fit,newdata = list(age=age.grid)),col="darkgreen",lwd=2,type="l")
#adding cutpoints
abline(v=c(25,40,60),lty=2,col="darkgreen")
#fitting smoothing splines using smooth.spline(X,Y,df=...)
fit1<-smooth.spline(age,wage,df=16) #16 degrees of freedom
#Plotting both cubic and Smoothing Splines
plot(age,wage,col="grey",xlab="Age",ylab="Wages")
points(age.grid,predict(fit,newdata = list(age=age.grid)),col="darkgreen",lwd=2,type="l")
#adding cutpoints
abline(v=c(25,40,60),lty=2,col="darkgreen")
lines(fit1,col="red",lwd=2)
legend("topright",c("Smoothing Spline with 16 df","Cubic Spline"),col=c("red","darkgreen"),lwd=2)
fit2<-smooth.spline(age,wage,cv = TRUE)
fit2
#It selects $\lambda=0.0279$ and df = 6.794596 as it is a Heuristic and can take various values for how rough the #function is
plot(age,wage,col="grey")
#Plotting Regression Line
lines(fit2,lwd=2,col="purple")
legend("topright",("Smoothing Splines with 6.78 df selected by CV"),col="purple",lwd=2)
if (!require("e1071")){install.packages("e1071",verbose = F) ; library("e1071")}
rm(list=ls())
set.seed(10111)
?rnorn
?rnorm
?matrix
x = matrix(rnorm(40), 20, 2)
rep(5,10)
rep(5,100)
rep(5,10*10)
rep(5,1010)
rep(5,1010)
1`1`
set.seed(10111)
x = matrix(rnorm(40), 20, 2)
y = rep(c(-1, 1), c(10,10))
x[y == 1,] = x[y == 1,] + 1
plot(x, col = y + 3, pch = 19)
dat = data.frame(x, y = as.factor(y))
data
dat
head(dat)
dat = data.frame(x, y = as.factor(y))
print(svmfit)
svmfit = svm(y~., data = dat, kernel = "linear", cost = 10, scale = FALSE)
print(svmfit)
svmfit = svm(y~., data = dat, kernel = "linear", cost = 20, scale = FALSE)
print(svmfit)
plot(svmfit, dat)
make.grid = function(x, n = 75) {
grange = apply(x, 2, range)
x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
expand.grid(X1 = x1, X2 = x2)
}
xgrid = make.grid(x)
xgrid[1:10,]
ygrid = predict(svmfit, xgrid)
plot(xgrid, col = c("red","blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
svmfit$kernel
svmfit$nclasses
svmfit$index
beta = drop(t(svmfit$coefs)%*%x[svmfit$index,])
beta0 = svmfit$rho
beta0
beta
plot(xgrid, col = c("red", "blue")[as.numeric(ygrid)], pch = 20, cex = .2)
points(x, col = y + 3, pch = 19)
points(x[svmfit$index,], pch = 5, cex = 2)
abline(beta0 / beta[2], -beta[1] / beta[2])
abline((beta0 - 1) / beta[2], -beta[1] / beta[2], lty = 2)
abline((beta0 + 1) / beta[2], -beta[1] / beta[2], lty = 2)
load(file = "ESL.mixture.rda")
load(file = "ESL.mixture.rda")
names(ESL.mixture)
rm(x, y)
x
y
rm(x, y)
attach(ESL.mixture)
x
y
ESL.mixture$x
x
x<-5
x
detach(ESL.mixture)
y
load(file = "ESL.mixture.rda")
names(ESL.mixture)
rm(x, y)
attach(ESL.mixture)
plot(x, col = y + 1)
dat = data.frame(y = factor(y), x)
fit = svm(y~., data = dat, scale = FALSE, kernel = "radial", cost = 5)
fit
xgrid = expand.grid(X1 = px1, X2 = px2)
ygrid = predict(fit, xgrid)
plot(xgrid, col = as.numeric(ygrid), pch = 20, cex = .2)
points(x, col = y + 1, pch = 19)
func = predict(fit, xgrid, decision.values = TRUE)
func = attributes(func)$decision
attributes(func)
attributes(func)$decision
func = predict(fit, xgrid, decision.values = TRUE)
func = attributes(func)$decision
xgrid = expand.grid(X1 = px1, X2 = px2)
ygrid = predict(fit, xgrid)
plot(xgrid, col = as.numeric(ygrid), pch = 20, cex = .2)
points(x, col = y + 1, pch = 19)
contour(px1, px2, matrix(func, 69, 99), level = 0, add = TRUE)
contour(px1, px2, matrix(func, 69, 99), level = 0.5, add = TRUE, col = "blue", lwd = 2)
attributes(func)$decision
xgrid = expand.grid(X1 = px1, X2 = px2)
ygrid = predict(fit, xgrid)
plot(xgrid, col = as.numeric(ygrid), pch = 20, cex = .2)
points(x, col = y + 1, pch = 19)
func = predict(fit, xgrid, decision.values = TRUE)
func = attributes(func)$decision
xgrid = expand.grid(X1 = px1, X2 = px2)
ygrid = predict(fit, xgrid)
plot(xgrid, col = as.numeric(ygrid), pch = 20, cex = .2)
points(x, col = y + 1, pch = 19)
contour(px1, px2, matrix(func, 69, 99), level = 0, add = TRUE)
